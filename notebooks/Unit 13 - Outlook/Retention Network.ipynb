{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Challenges\n",
    "Retentive Network (RetNet) is a foundational architecture proposed for large language models in the paper [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621). This architecture is designed to address key challenges in the realm of large-scale language modeling: training parallelism, low-cost inference, and good performance.\n",
    "\n",
    "* **Training parallelism**: During the training, all of the input tokens are processed at the same time, utilizing the GPUs parallel processing power. One example of this is the Transformer architecture; where the self-attention inside the decoder allows token generation that does not depend on the previously generated output. \n",
    "* **Low-cost inference**: During the inference, the cost does not scale with sequence length. One example of this is the Recurrent Neural Network (RNN) architecture; where it uses simple and cheap operation like matrix multiplication to process one token at each time step.\n",
    "* **Good performance**: Transformer and RNN are both excellent, but Transformer has high-cost inference and RNN's training is not parallelizable. On the other hand, linear transformers are parallelizable and its inference is made cheap by sequential processing, but it has poor performance.\n",
    "\n",
    "RetNet addresses all of these challenges thanks to its multi-scale retention mechanism, which will be explained below.\n",
    "\n",
    "Read more about these explanations [here](https://medium.com/ai-fusion-labs/retentive-networks-retnet-explained-the-much-awaited-transformers-killer-is-here-6c17e3e8add8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetNet Architecture\n",
    "\n",
    "<center><img src=\"Retention_imgs/RetNet Architecture.jpg\" style=\"height: 400px; width:auto;\"></center>\n",
    "\n",
    "\n",
    "Retentive Network has a similar architecture to the Transformer's encoder, but with a few differences:\n",
    "    <ol>\n",
    "    <li> The encoder precedes the Feed Forward Network (FFN) and token mixer layer (multi-scale retention (MSR) layer).\n",
    "    <li> The multi-scale retention layer (the proposed method) replaces the multi-head attention layer.\n",
    "    </ol>\n",
    "    \n",
    "The input sequence ${x}^{|x|}_{i=1}$ is transformed to vectors by a word embedding layer and then packed to form a matrix $X^0=[x_1, \\cdot\\cdot\\cdot, x_{|x|}] \\in \\mathbb{R}^{|x|\n",
    "\\times d_{model}}$. The computation of the output of the $l$-th layer is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    Y^l &= \\text{MSR}(\\text{LN}(X^l)) + X^l \\\\\n",
    "    X^{l+1} &= \\text{FFN}(\\text{LN}(Y^l) + Y^l\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\text{LN}$ is LayerNorm and $\\text{FFN}$ is the feed forward network computed as $\\text{FFN}(X) = \\text{gelu}(XW_1)W_2$ where $W_1$ and $W_2$ are learnable parameters. \n",
    "\n",
    "Read more:\n",
    "- [Embedding layer](https://lena-voita.github.io/nlp_course/word_embeddings.html)\n",
    "- [gelu](https://arxiv.org/pdf/1606.08415v5.pdf)\n",
    "- [LayerNorm](https://arxiv.org/abs/1607.06450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale Retention \n",
    "\n",
    "<center><img src=\"Retention_imgs/Multi Scale Retention.jpg\" style=\"height: 800px; width:auto;\"></center>\n",
    "\n",
    "The detail of the computation of the multi-scale retention layer is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma &= 1-2^{-5-arange(0,h)} \\\\\n",
    "    head_i &= \\text{Retention}(X, \\gamma_i) \\\\\n",
    "    Y &= GroupNorm_h(Concat(head_1, \\cdot\\cdot\\cdot, head_h)) \\\\\n",
    "    \\text{MSR}(X) &= (swish(XW_G) \\odot Y)W_O\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $h=\\frac{d_{model}}{d}$ is the number of heads, $d$ is the head dimension, $arange(0,h)$ is the range of integers from 0 to $h$, $\\gamma$ is the retention scale, $head_i$ is the output of the $i$-th head, $GroupNorm_h$ is the group normalization applied on each head, $\\odot$ is the element-wise multiplication, $W_G$ and $W_O$ are learnable parameters, and $swish$ is the swish activation function.\n",
    "\n",
    "Let's consider a case like in the figure above, where $h=3$, $|x|=2$, and $d_{model}=4$ (the dimension of the head is usually an integer as $d_{model}$ is the multiplication $d$. Here, $d_{model}=4$ is used only for illustration purpose). Since there are three heads, there will be three different value of gamma $\\gamma_1=1-2^{-5-0}=0.96875$, $\\gamma_2=1-2^{-5-1}=0.9375$, and $\\gamma_3=1-2^{-5-2}=0.875$ applied on each head respectively. However, these gammas are fixed and identical among different layers.\n",
    "\n",
    "Then the input will go through each retention head. Each head's output will be fed to the GroupNorm layer. The GroupNorm layer is applied on each head separately because the heads use multiple $\\gamma$, resulting each head has diferent variance statistics that need to be normalized. After that, the results are concatenated and element-wise multiplied with the output of the swish gate to increase the non-linearity of the retention layers. Finally, the output is projected by multipliying it with $W_O$ so that the output has the same dimension as the input, which is $2 \\times 4$.\n",
    "\n",
    "Read more:\n",
    "- [GroupNorm](https://arxiv.org/abs/1803.08494)\n",
    "- [swish](https://arxiv.org/pdf/1710.05941v1.pdf?source=post_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "\n",
    "def rotate_every_two(x):\n",
    "    x1 = x[:, :, :, ::2]\n",
    "    x2 = x[:, :, :, 1::2]\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n",
    "\n",
    "def theta_shift(x, sin, cos):\n",
    "    return (x * cos) + (rotate_every_two(x) * sin)\n",
    "\n",
    "def get_activation_fn(activation):\n",
    "    if activation == \"swish\":\n",
    "        return F.silu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def MultiwayWrapper(args, module, dim=1):\n",
    "    if args.multiway:\n",
    "        return MultiwayNetwork(module, dim=dim)\n",
    "    return module\n",
    "\n",
    "class MultiwayNetwork(nn.Module):\n",
    "    def __init__(self, module, dim=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.A = module\n",
    "        self.B = copy.deepcopy(module)\n",
    "        self.B.reset_parameters()\n",
    "        self.split_position = -1\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.split_position == -1:\n",
    "            return self.A(x, **kwargs)\n",
    "        if self.split_position == 0:\n",
    "            return self.B(x, **kwargs)\n",
    "        x1, x2 = torch.split(\n",
    "            x,\n",
    "            [self.split_position, x.size(self.dim) - self.split_position],\n",
    "            dim=self.dim,\n",
    "        )\n",
    "        # x1, x2 = x[:self.split_position], x[self.split_position:]\n",
    "        y1, y2 = self.A(x1, **kwargs), self.B(x2, **kwargs)\n",
    "        return torch.cat([y1, y2], dim=self.dim)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(dim))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        if self.weight is not None:\n",
    "            output = output * self.weight\n",
    "        return output\n",
    "\n",
    "class MultiScaleRetention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        embed_dim,\n",
    "        value_dim,\n",
    "        num_heads,\n",
    "        gate_fn=\"swish\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.embed_dim = embed_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.value_dim // num_heads\n",
    "        self.key_dim = self.embed_dim // num_heads\n",
    "        self.scaling = self.key_dim ** -0.5\n",
    "        \n",
    "        self.gate_fn = get_activation_fn(activation=str(gate_fn))\n",
    "\n",
    "        self.q_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "        self.k_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "        self.v_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "        self.g_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "        \n",
    "        self.out_proj = MultiwayWrapper(args, nn.Linear(value_dim, embed_dim, bias=False))\n",
    "\n",
    "        self.group_norm = MultiwayWrapper(args, RMSNorm(self.head_dim, eps=args.layernorm_eps, elementwise_affine=False))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.g_proj.weight, gain=2 ** -2.5)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight, gain=2 ** -1)\n",
    "\n",
    "    def parallel_forward(self, qr, kr, v, mask):\n",
    "        bsz, tgt_len, embed_dim = v.size()\n",
    "\n",
    "        vr = v.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        qk_mat = qr @ kr.transpose(-1, -2) # bsz * m * tgt_len * tgt_len\n",
    "        qk_mat = qk_mat * mask\n",
    "        # invariant after normalization\n",
    "        qk_mat = qk_mat / qk_mat.detach().abs().sum(dim=-1, keepdim=True).clamp(min=1, max=5e4)\n",
    "        output = torch.matmul(qk_mat, vr)\n",
    "        output = output.transpose(1, 2)\n",
    "        return output\n",
    "\n",
    "    def recurrent_forward(\n",
    "        self,\n",
    "        qr, kr, v,\n",
    "        decay,\n",
    "        incremental_state\n",
    "    ):\n",
    "        bsz = v.size(0)\n",
    "\n",
    "        v = v.view(bsz, self.num_heads, self.head_dim, 1)\n",
    "        kv = kr * v\n",
    "        if \"prev_key_value\" in incremental_state:\n",
    "            prev_kv = incremental_state[\"prev_key_value\"]\n",
    "            prev_scale = incremental_state[\"scale\"]\n",
    "            scale = prev_scale * decay + 1\n",
    "            kv = prev_kv * (prev_scale.sqrt() * decay / scale.sqrt()).view(self.num_heads, 1, 1) + kv / scale.sqrt().view(self.num_heads, 1, 1)\n",
    "            # kv = prev_kv * decay.view(self.num_heads, 1, 1) + kv\n",
    "        else:\n",
    "            scale = torch.ones_like(decay)\n",
    "\n",
    "        incremental_state[\"prev_key_value\"] = kv\n",
    "        incremental_state[\"scale\"] = scale\n",
    "\n",
    "        output = torch.sum(qr * kv, dim=3)\n",
    "        return output\n",
    "    \n",
    "    def chunk_recurrent_forward(\n",
    "        self,\n",
    "        qr, kr, v,\n",
    "        inner_mask\n",
    "    ):\n",
    "        mask, cross_decay, query_inner_decay, value_inner_decay = inner_mask\n",
    "        bsz, tgt_len, embed_dim = v.size()\n",
    "        chunk_len = mask.size(1)\n",
    "        num_chunks = tgt_len // chunk_len\n",
    "\n",
    "        assert tgt_len % chunk_len == 0\n",
    "\n",
    "        qr = qr.view(bsz, self.num_heads, num_chunks, chunk_len, self.key_dim).transpose(1, 2)\n",
    "        kr = kr.view(bsz, self.num_heads, num_chunks, chunk_len, self.key_dim).transpose(1, 2)\n",
    "        v = v.view(bsz, num_chunks, chunk_len, self.num_heads, self.head_dim).transpose(2, 3)\n",
    "\n",
    "        kr_t = kr.transpose(-1, -2)\n",
    "\n",
    "        qk_mat = qr @ kr_t # bsz * num_heads * chunk_len * chunk_len\n",
    "        qk_mat = qk_mat * mask\n",
    "        inner_scale = qk_mat.detach().abs().sum(dim=-1, keepdim=True).clamp(min=1)\n",
    "        qk_mat = qk_mat / inner_scale\n",
    "        inner_output = torch.matmul(qk_mat, v) # bsz * num_heads * num_value_heads * chunk_len * head_dim\n",
    "        \n",
    "        # reduce kv in one chunk\n",
    "        kv = kr_t @ (v * value_inner_decay)\n",
    "\n",
    "        kv_recurrent = []\n",
    "        cross_scale = []\n",
    "        kv_state = torch.zeros(bsz, self.num_heads, self.key_dim, self.head_dim).to(v)\n",
    "        kv_scale = torch.ones(bsz, self.num_heads, 1, 1).to(v)\n",
    "        \n",
    "        # accumulate kv by loop\n",
    "        for i in range(num_chunks):\n",
    "            kv_recurrent.append(kv_state / kv_scale)\n",
    "            cross_scale.append(kv_scale)\n",
    "            kv_state = kv_state * cross_decay + kv[:, i]\n",
    "            kv_scale = kv_state.detach().abs().sum(dim=-2, keepdim=True).max(dim=-1, keepdim=True).values.clamp(min=1)\n",
    "            \n",
    "        kv_recurrent = torch.stack(kv_recurrent, dim=1)\n",
    "        cross_scale = torch.stack(cross_scale, dim=1)\n",
    "        \n",
    "        all_scale = torch.maximum(inner_scale, cross_scale)\n",
    "        align_inner_scale = all_scale / inner_scale\n",
    "        align_cross_scale = all_scale / cross_scale\n",
    "\n",
    "        cross_output = (qr * query_inner_decay) @ kv_recurrent\n",
    "        output = inner_output / align_inner_scale + cross_output / align_cross_scale\n",
    "        # output = inner_output / cross_scale + cross_output / inner_scale\n",
    "\n",
    "        output = output.transpose(2, 3)\n",
    "        return output\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        rel_pos,\n",
    "        chunkwise_recurrent=False,\n",
    "        incremental_state=None\n",
    "    ):\n",
    "        bsz, tgt_len, _ = x.size()\n",
    "        (sin, cos), inner_mask = rel_pos\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        g = self.g_proj(x)\n",
    "\n",
    "        k *= self.scaling\n",
    "        q = q.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "\n",
    "        qr = theta_shift(q, sin, cos)\n",
    "        kr = theta_shift(k, sin, cos)\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            output = self.recurrent_forward(qr, kr, v, inner_mask, incremental_state)\n",
    "        elif chunkwise_recurrent:\n",
    "            output = self.chunk_recurrent_forward(qr, kr, v, inner_mask)\n",
    "        else:\n",
    "            output = self.parallel_forward(qr, kr, v, inner_mask)\n",
    "        \n",
    "        output = self.group_norm(output).reshape(bsz, tgt_len, self.head_dim * self.num_heads)\n",
    "\n",
    "        output = self.gate_fn(g) * output\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Going line by line in the `forward` function:\n",
    "**Variables**\n",
    "These are important variables that are going to be used along the computation:\n",
    "1. Batch size (bsz): The batch size of the input.\n",
    "2. Target len / Sequence len (tgt_len): The length of the sequence.\n",
    "3. (sin, cos): Angle for positional embedding.\n",
    "4. inner_mask: Masking matrix; depends on the mode used, it can also contain other constants.\n",
    "```python\n",
    "bsz, tgt_len, _ = x.size()\n",
    "(sin, cos), inner_mask = rel_pos\n",
    "```\n",
    "\n",
    "**Key, Query, Value**\n",
    "Obtain the key, query, and value representation of the input by multiplying the input with learnable matrices.\n",
    "```python\n",
    "q = self.q_proj(x)\n",
    "k = self.k_proj(x)\n",
    "v = self.v_proj(x)\n",
    "g = self.g_proj(x)\n",
    "```\n",
    "They are declared in the `__init__` function\n",
    "```python\n",
    "self.q_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "self.k_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=False))\n",
    "self.v_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "self.g_proj = MultiwayWrapper(args, nn.Linear(embed_dim, value_dim, bias=False))\n",
    "```\n",
    "\n",
    "**Multi-Head**\n",
    "Dividing the query and key matrices into several heads (similar to Multi-Head Attention).\n",
    "```python\n",
    "k *= self.scaling\n",
    "q = q.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "k = k.view(bsz, tgt_len, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "```\n",
    "The number of heads and key dimension are declared in the `__init__` function.\n",
    "```python\n",
    "self.num_heads = num_heads\n",
    "self.head_dim = self.value_dim // num_heads\n",
    "self.key_dim = self.embed_dim // num_heads\n",
    "```\n",
    "The multi-head operation will also be applied to the value matrix, but it is implemented inside the represenation calculation (parallel, recurrent, chunkwise recurrent).\n",
    "\n",
    "**Positional Embedding**\n",
    "Add the Extrapolatable Position Embedding [XPos](https://arxiv.org/abs/2212.10554).\n",
    "```python\n",
    "qr = theta_shift(q, sin, cos)\n",
    "kr = theta_shift(k, sin, cos)\n",
    "```\n",
    "where `theta_shift` function is defined outside the class.\n",
    "```python\n",
    "def rotate_every_two(x):\n",
    "    x1 = x[:, :, :, ::2]\n",
    "    x2 = x[:, :, :, 1::2]\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n",
    "\n",
    "def theta_shift(x, sin, cos):\n",
    "    return (x * cos) + (rotate_every_two(x) * sin)\n",
    "```\n",
    "\n",
    "**Representation Calculation**\n",
    "Multi-Scale Retention performs computation according to the condition.\n",
    "```python\n",
    "if incremental_state is not None:\n",
    "    output = self.recurrent_forward(qr, kr, v, inner_mask, incremental_state)\n",
    "elif chunkwise_recurrent:\n",
    "    output = self.chunk_recurrent_forward(qr, kr, v, inner_mask)\n",
    "else:\n",
    "    output = self.parallel_forward(qr, kr, v, inner_mask)\n",
    "```\n",
    "where each representation computation is declared inside the MultiScaleRetention class (will be explained later).\n",
    "\n",
    "**Normalization, Gating, and Projection**\n",
    "Perform normalization, apply gating function, and do projection.\n",
    "```python\n",
    "output = self.group_norm(output).reshape(bsz, tgt_len, self.head_dim * self.num_heads)\n",
    "output = self.gate_fn(g) * output\n",
    "output = self.out_proj(output)\n",
    "return output\n",
    "```\n",
    "where each function is declared in the `__init__` function\n",
    "```python\n",
    "self.gate_fn = get_activation_fn(activation=str(gate_fn))\n",
    "self.out_proj = MultiwayWrapper(args, nn.Linear(value_dim, embed_dim, bias=False))\n",
    "self.group_norm = MultiwayWrapper(args, RMSNorm(self.head_dim, eps=args.layernorm_eps, elementwise_affine=False))\n",
    "```\n",
    "and `get_activation_fn` is declared outside the class\n",
    "```python\n",
    "def get_activation_fn(activation):\n",
    "    if activation == \"swish\":\n",
    "        return F.silu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
