# Supplementary Reading and Resources ðŸ¤—

We hope that you found the unit on multimodal models exciting. If you'd like to learn and explore in detail about multimodal learning and models, here is a list of resources for your reference:

- [**Hugging Face Tasks**](https://huggingface.co/tasks) offers an overview of various tasks under domains like Computer Vision, Audio, NLP, Multimodal Learning and Reinforcement Learning. The tasks contain demos, use cases, models, datasets, etc.
- [**11-777 MMML**](https://cmu-multicomp-lab.github.io/mmml-course/fall2022/) course on multimodal machine learning by CMU. You can find the video lectures [**here**](https://www.youtube.com/@LPMorency/playlists).
- [**Blog on Multimodality and LLMs by Chip Huyen**](https://huyenchip.com/2023/10/10/multimodal.html) provides a comprehensive overview of multimodality, large multimodal models, systems like BLIP, CLIP, etc.
- [**Awesome Multimodal ML**](https://github.com/pliang279/awesome-multimodal-ml), a GitHub repository containing papers, courses, architectures, workshops, tutorials etc.
- [**Awesome Multimodal Large Language Models**](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models), a GitHub repository containing papers and datasets related to multimodal LLMs. 
- [**EE/CS 148, Caltech**](https://gkioxari.github.io/teaching/cs148/) course on Large Language and Vision Models.
